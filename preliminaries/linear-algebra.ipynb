{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([5.]), tensor([6.]), tensor([1.5000]), tensor([9.]))"
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "标量\n",
    "'''\n",
    "\n",
    "x = torch.tensor([3.0])\n",
    "y = torch.tensor([2.0])\n",
    "\n",
    "x + y, x * y, x / y, x**y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3])\n",
      "tensor(3)\n",
      "4\n",
      "torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "向量\n",
    "(粗体小写)\n",
    "'''\n",
    "x = torch.arange(4)\n",
    "print(x)\n",
    "\n",
    "print(x[3])\n",
    "\n",
    "'长度'\n",
    "print(len(x))\n",
    "'形状'\n",
    "print(x.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11],\n",
      "        [12, 13, 14, 15],\n",
      "        [16, 17, 18, 19]])\n",
      "tensor([[ 0,  4,  8, 12, 16],\n",
      "        [ 1,  5,  9, 13, 17],\n",
      "        [ 2,  6, 10, 14, 18],\n",
      "        [ 3,  7, 11, 15, 19]])\n",
      "tensor([[1, 2, 3],\n",
      "        [2, 0, 4],\n",
      "        [3, 4, 5]])\n",
      "tensor([[True, True, True],\n",
      "        [True, True, True],\n",
      "        [True, True, True]])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "矩阵\n",
    "(粗体大写)\n",
    "'''\n",
    "A = torch.arange(20).reshape(5,-1)\n",
    "print(A)\n",
    "\n",
    "'矩阵的转置'\n",
    "B = A.T\n",
    "print(B)\n",
    "\n",
    "\n",
    "C = torch.tensor([[1, 2, 3], [2, 0, 4], [3, 4, 5]])\n",
    "print(C)\n",
    "print(C == C.T)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0,  1,  2,  3],\n",
      "         [ 4,  5,  6,  7],\n",
      "         [ 8,  9, 10, 11]],\n",
      "\n",
      "        [[12, 13, 14, 15],\n",
      "         [16, 17, 18, 19],\n",
      "         [20, 21, 22, 23]]])\n",
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.],\n",
      "        [12., 13., 14., 15.],\n",
      "        [16., 17., 18., 19.]])\n",
      "tensor([[ 0.,  2.,  4.,  6.],\n",
      "        [ 8., 10., 12., 14.],\n",
      "        [16., 18., 20., 22.],\n",
      "        [24., 26., 28., 30.],\n",
      "        [32., 34., 36., 38.]])\n",
      "tensor([[  0.,   1.,   4.,   9.],\n",
      "        [ 16.,  25.,  36.,  49.],\n",
      "        [ 64.,  81., 100., 121.],\n",
      "        [144., 169., 196., 225.],\n",
      "        [256., 289., 324., 361.]])\n",
      "tensor([[[ 2,  3,  4,  5],\n",
      "         [ 6,  7,  8,  9],\n",
      "         [10, 11, 12, 13]],\n",
      "\n",
      "        [[14, 15, 16, 17],\n",
      "         [18, 19, 20, 21],\n",
      "         [22, 23, 24, 25]]])\n",
      "tensor([[[ 0,  2,  4,  6],\n",
      "         [ 8, 10, 12, 14],\n",
      "         [16, 18, 20, 22]],\n",
      "\n",
      "        [[24, 26, 28, 30],\n",
      "         [32, 34, 36, 38],\n",
      "         [40, 42, 44, 46]]])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "张量\n",
    "'''\n",
    "\n",
    "X = torch.arange(24).reshape((2,3,4))\n",
    "print(X)\n",
    "\n",
    "A = torch.arange(20,dtype=torch.float32).reshape(5,4)\n",
    "B = A.clone()   # 通过分配新内存，将A的⼀个副本分配给B\n",
    "\n",
    "print(A)\n",
    "print(A + B)\n",
    "\n",
    "print(A * B) #哈达玛积（２个形状相同的矩阵相乘）\n",
    "\n",
    "#张量和标量的相加和相乘\n",
    "a = 2\n",
    "X = torch.arange(24).reshape(2,3,4)\n",
    "print(a + X)\n",
    "print(a * X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2., 3.])\n",
      "tensor(6.)\n",
      "torch.Size([5, 4])\n",
      "tensor(190.)\n",
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.],\n",
      "        [12., 13., 14., 15.],\n",
      "        [16., 17., 18., 19.]])\n",
      "tensor([40., 45., 50., 55.])\n",
      "torch.Size([4])\n",
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.],\n",
      "        [12., 13., 14., 15.],\n",
      "        [16., 17., 18., 19.]])\n",
      "tensor([ 6., 22., 38., 54., 70.])\n",
      "torch.Size([5])\n",
      "tensor(190.)\n",
      "tensor(9.5000)\n",
      "tensor(9.5000)\n",
      "tensor([ 8.,  9., 10., 11.])\n",
      "tensor([ 8.,  9., 10., 11.])\n"
     ]
    }
   ],
   "source": [
    "'''降维'''\n",
    "\n",
    "#向量的求和函数\n",
    "x = torch.arange(4,dtype=torch.float32)\n",
    "print(x)\n",
    "print(x.sum())\n",
    "\n",
    "#矩阵的求和\n",
    "print(A.shape)\n",
    "print(A.sum())\n",
    "\n",
    "#输⼊的轴0的维数在输出形状中丢失\n",
    "A_sum_axis0 = A.sum(axis=0)\n",
    "print(A)\n",
    "print(A_sum_axis0)\n",
    "print(A_sum_axis0.shape)\n",
    "#输⼊的轴1的维数在输出形状中消失\n",
    "A_sum_axis1 = A.sum(axis=1)\n",
    "print(A)\n",
    "print(A_sum_axis1)\n",
    "print(A_sum_axis1.shape)\n",
    "\n",
    "#对张量所有轴求和,相当与对所有元素求和\n",
    "print(A.sum(axis = [0,1]))\n",
    "\n",
    "#与求和相关的求均值\n",
    "print(A.mean())\n",
    "print(A.sum() / A.numel())\n",
    "#沿轴计算均值\n",
    "print(A.mean(axis=0))\n",
    "print(A.sum(axis=0) / A.shape[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.],\n",
      "        [12., 13., 14., 15.],\n",
      "        [16., 17., 18., 19.]])\n",
      "tensor([[ 6.],\n",
      "        [22.],\n",
      "        [38.],\n",
      "        [54.],\n",
      "        [70.]])\n",
      "tensor([[0.0000, 0.1667, 0.3333, 0.5000],\n",
      "        [0.1818, 0.2273, 0.2727, 0.3182],\n",
      "        [0.2105, 0.2368, 0.2632, 0.2895],\n",
      "        [0.2222, 0.2407, 0.2593, 0.2778],\n",
      "        [0.2286, 0.2429, 0.2571, 0.2714]])\n",
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  6.,  8., 10.],\n",
      "        [12., 15., 18., 21.],\n",
      "        [24., 28., 32., 36.],\n",
      "        [40., 45., 50., 55.]])\n"
     ]
    }
   ],
   "source": [
    "'''非降维求和'''\n",
    "sum_A = A.sum(axis=1,keepdim=True)\n",
    "print(A)\n",
    "print(sum_A)\n",
    "\n",
    "#于sum_A在对每⾏进⾏求和后仍保持两个轴，我们可以通过⼴播将A除以sum_A。\n",
    "print(A / sum_A)\n",
    "#计算累积和　不改变张量的维度\n",
    "print(A.cumsum(axis=0))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2., 3.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor(6.)\n",
      "tensor(6.)\n",
      "tensor(6.)\n",
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.],\n",
      "        [12., 13., 14., 15.],\n",
      "        [16., 17., 18., 19.]])\n",
      "tensor([0., 1., 2., 3.])\n",
      "torch.Size([4])\n",
      "tensor([ 14.,  38.,  62.,  86., 110.])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.],\n",
      "        [12., 13., 14., 15.],\n",
      "        [16., 17., 18., 19.]])\n",
      "tensor([[ 6.,  6.,  6.],\n",
      "        [22., 22., 22.],\n",
      "        [38., 38., 38.],\n",
      "        [54., 54., 54.],\n",
      "        [70., 70., 70.]])\n"
     ]
    }
   ],
   "source": [
    "'''点积'''\n",
    "\n",
    "#向量的点积\n",
    "y = torch.ones(4,dtype=torch.float32)\n",
    "print(x)\n",
    "print(y)\n",
    "print(torch.dot(x,y))\n",
    "print((x.T * y).sum())\n",
    "print(torch.sum(x * y))\n",
    "\n",
    "#向量与矩阵的点积\n",
    "print(A)\n",
    "print(x)\n",
    "print(x.shape)\n",
    "print(torch.mv(A,x))\n",
    "\n",
    "#矩阵乘积\n",
    "B = torch.ones(4,3)\n",
    "print(B)\n",
    "print(A)\n",
    "print(torch.mm(A,B))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.)\n",
      "tensor(7.)\n",
      "tensor(6.)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "范数\n",
    "向量表⽰物品(如单词、产\n",
    "品或新闻⽂章)，以便最小化相似项⽬之间的距离，最⼤化不同项⽬之间的距离。通常，⽬标，或许是深度学\n",
    "习算法最重要的组成部分(除了数据)，被表达为范数\n",
    "'''\n",
    "\n",
    "'''\n",
    "向量范数是将向量映射到标量的函数f 范数\n",
    "范数的性质\n",
    "１．f(αx) = |α|f(x)\n",
    "２．f(x + y) ≤ f(x) + f(y)\n",
    "３．f(x) ≥ 0\n",
    "L2范数：元素平方和开根号(欧基里德距离)\n",
    "L1范数：元素绝对值之和\n",
    "与L2范数相⽐，L1范数受异常值的影响较小\n",
    "\n",
    "\n",
    "弗罗⻉尼乌斯范数(矩阵范数)：矩阵所有元素平方和的开方\n",
    "'''\n",
    "u = torch.tensor([3.0, -4.0])\n",
    "\n",
    "L1 = torch.norm(u)\n",
    "print(L1)\n",
    "L2 = torch.abs(u).sum()\n",
    "print(L2)\n",
    "\n",
    "Lf = torch.norm(torch.ones((4,9)))\n",
    "print(Lf)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}