{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n",
      "torch.Size([12])\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "1维：向量(Vector)\n",
    "2维：矩阵(Matrix)\n",
    "3+维：张量\n",
    "'''\n",
    "x = torch.arange(12)\n",
    "print(x)\n",
    "'形状'\n",
    "print(x.shape)\n",
    "'元素个数'\n",
    "print(x.numel())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n",
      "torch.Size([3, 4])\n",
      "12\n",
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n",
      "torch.Size([3, 4])\n",
      "12\n",
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n",
      "torch.Size([3, 4])\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "reshape改变张量的形状,而不改变元素的个数和值\n",
    "'''\n",
    "X = x.reshape(3,4)\n",
    "print(X)\n",
    "print(X.shape)\n",
    "print(X.numel())\n",
    "\n",
    "X = x.reshape(-1,4)\n",
    "print(X)\n",
    "print(X.shape)\n",
    "print(X.numel())\n",
    "\n",
    "X = x.reshape(3,-1)\n",
    "print(X)\n",
    "print(X.shape)\n",
    "print(X.numel())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]]])\n",
      "tensor([[[1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.]]])\n",
      "tensor([[ 2.9281e-01, -1.0543e+00, -1.0282e+00,  1.0927e+00],\n",
      "        [-2.5435e-01, -1.9436e-01, -1.0356e+00, -1.0798e+00],\n",
      "        [ 5.3275e-02, -9.8743e-04, -1.2364e+00,  9.9904e-01]])\n"
     ]
    }
   ],
   "source": [
    "tensor_zeros = torch.zeros((2,3,4))\n",
    "tensor_ones = torch.ones((2,3,4))\n",
    "'符合均值为０,标准差为１的高斯分布随机采样'\n",
    "tensor_randn = torch.randn(3,4)\n",
    "print(tensor_zeros)\n",
    "print(tensor_ones)\n",
    "print(tensor_randn)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[2, 1, 4, 3],\n        [1, 2, 3, 4],\n        [4, 3, 2, 1]])"
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'最外层的列表对应于轴 0，内层的列表对应于轴 1'\n",
    "torch.tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 3.,  4.,  6., 10.])\n",
      "tensor([-1.,  0.,  2.,  6.])\n",
      "tensor([ 2.,  4.,  8., 16.])\n",
      "tensor([0.5000, 1.0000, 2.0000, 4.0000])\n",
      "tensor([ 1.,  4., 16., 64.])\n",
      "tensor([2.7183e+00, 7.3891e+00, 5.4598e+01, 2.9810e+03])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "运算：\n",
    "对于任意具有相同形状的张量，常⻅的标准算术运算符（+、-、*、/ 和 **(求幂)）都可以被升级为按元素运算\n",
    "'''\n",
    "x = torch.tensor([1.0, 2, 4, 8])\n",
    "y = torch.tensor([2, 2, 2, 2])\n",
    "\n",
    "print(x + y)\n",
    "print(x - y)\n",
    "print(x * y)\n",
    "print(x / y)\n",
    "print(x ** y)\n",
    "print(torch.exp(x))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 4., 8.])\n",
      "tensor([2, 2, 2, 2])\n",
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.],\n",
      "        [ 2.,  1.,  4.,  3.],\n",
      "        [ 1.,  2.,  3.,  4.],\n",
      "        [ 4.,  3.,  2.,  1.]])\n",
      "tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],\n",
      "        [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "张量合并\n",
    "'''\n",
    "X = torch.arange(12,dtype=torch.float32).reshape((3,4))\n",
    "print(x)\n",
    "Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\n",
    "print(y)\n",
    "\n",
    "print(torch.cat((X,Y),dim=0))\n",
    "print(torch.cat((X,Y),dim=1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[False,  True, False,  True],\n",
      "        [False, False, False, False],\n",
      "        [False, False, False, False]])\n",
      "tensor(66.)\n"
     ]
    }
   ],
   "source": [
    "'逐元素比较'\n",
    "print(X == Y)\n",
    "'元素个数'\n",
    "print(X.sum())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0],\n",
      "        [1],\n",
      "        [2]])\n",
      "tensor([0, 1])\n",
      "tensor([[0, 1],\n",
      "        [1, 2],\n",
      "        [2, 3]])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "广播机制：\n",
    "形状不同的张量间进行计算\n",
    "'''\n",
    "\n",
    "a = torch.arange(3).reshape(3,1)\n",
    "b = torch.arange(2).reshape(2)\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "'矩阵 a将复制列，矩阵 b将复制⾏，然后再按元素相加'\n",
    "print(a + b)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.]])\n",
      "tensor([ 8.,  9., 10., 11.])\n",
      "tensor([[ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.]])\n",
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  9.,  7.],\n",
      "        [ 8.,  9., 10., 11.]])\n",
      "tensor([[12., 12., 12., 12.],\n",
      "        [12., 12., 12., 12.],\n",
      "        [ 8.,  9., 10., 11.]])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "索引和切片：\n",
    "'''\n",
    "print(X)\n",
    "'最后一个元素'\n",
    "print(X[-1])\n",
    "print(X[1:3])\n",
    "\n",
    "X[1,2] = 9\n",
    "print(X)\n",
    "'多元素操作'\n",
    "X[0:2,:] = 12\n",
    "print(X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139982686273088\n",
      "139982686273088\n",
      "True\n",
      "id(Z): 139982660088128\n",
      "id(Z): 139982660088128\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "节省内存\n",
    "'''\n",
    "before = id(Y)\n",
    "'Python 首先计算 Y + X，为结果分配新的内存，然后使 Y 指向内存中的这个新位置'\n",
    "#Y = X + Y\n",
    "'使用切片法 结果内存地址没有改变'\n",
    "Y[:] = X + Y\n",
    "after = id(Y)\n",
    "\n",
    "print(before)\n",
    "print(after)\n",
    "print(before==after)\n",
    "\n",
    "'''\n",
    "\n",
    "1.机器学习中可能有数百兆的参数 我们需要在１秒内原地更新所有参数\n",
    "2.很多其他引用指向旧的内存地址,如果不原地更新,某些代码可能会引用旧的参数\n",
    "'''\n",
    "\n",
    "Z = torch.zeros_like(Y)\n",
    "print('id(Z):', id(Z))\n",
    "'可以使用切片法将操作的结果分配给先前分配的数组 Y[:] = <expression>'\n",
    "Z[:] = X + a\n",
    "print('id(Z):', id(Z))\n",
    "\n",
    "\n",
    "'如果后面Ｘ不会复用 也可以使用X[:] = X + Y 或 X += Y 来减少操作的内存开销'\n",
    "before = id(X)\n",
    "#X[:] = X + Y\n",
    "X += Y\n",
    "id(X) == before"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'torch.Tensor'>\n",
      "False\n"
     ]
    },
    {
     "data": {
      "text/plain": "(tensor([3.5000]), 3.5, 3.5, 3)"
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "转换为其他python对象\n",
    "内存不共享\n",
    "'''\n",
    "\n",
    "A = X.numpy()\n",
    "print(id(A) == id(X))\n",
    "B = torch.tensor(A)\n",
    "print(type(A))\n",
    "print(type(B))\n",
    "print(id(A) == id(B))\n",
    "\n",
    "'将大小为１的张量转化为python标量 可以使用item函数'\n",
    "a = torch.tensor([3.5])\n",
    "a, a.item(), float(a), int(a)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}